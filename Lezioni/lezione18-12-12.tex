\section{Lezione del 12/12/2018 [Tantari]}

\subsection{Opinion formation}
\textcolor{red}{Mancante}

\subsection{Formalismo della meccanica statistica}
Sia $ (\Sigma, \P(\Sigma), P) $ uno spazio di probabilità, dove $ \Sigma $ è finito. Ogni elemento $ \sigma $ di $ \Sigma $ descrive un possibile stato del sistema.
Essendo lo spazio per l'appunto di dimensione finita, le sa $ \sigma $-algebra l'insieme delle parti, è sufficiente definire la misura di probabilità sui $ \sigma \in \Sigma $ per determinare $ P $ su ogni sottoinsieme di $ \sigma $.  Nel seguito quindi, con lieve abuso di notazione, scriveremo $ P(\sigma) $ intendendo $ P(\{\sigma\}) $. \\
Se il sistema viene descritto da una funzione \emph{hamiltoniana} $ \ham \colon \Sigma\to\R $ si introduce solitamente una misura di probabilità su $ \Sigma $, detta \emph{misura di Boltzmann}, data da
\begin{equation}
    P_\beta(\sigma) \coloneqq \frac{1}{Z_\beta} e^{-\beta\ham(\sigma)}
\end{equation}
dove è stata introdotta la \emph{funzione di partizione} del sistema
\begin{equation}
    Z_\beta \coloneqq \sum_{\sigma\in\Sigma} e^{-\beta \ham(\sigma)}.
\end{equation}
Muniti di questa misura possiamo esprimere il valor medio di una generica osservabile $ \mathcal{O}\colon \Sigma\to\R $ su tutti gli stati possibili:
\begin{equation} \label{eqn:media-osservabile}
    \langle\mathcal{O}\rangle = \sum_{\sigma\in\Sigma} P_\beta(\sigma)\mathcal{O}(\sigma).
\end{equation}
Uno degli assunti fondamentali della meccanica statistica è che sullo spazio $ (\Sigma, \P(\Sigma), P) $ sia definita una qualche dinamica misurabile e che il sistema dinamico risultante sia \emph{ergodico}. A questo punto ci si dimentica dell'esistenza di una dinamica e si utilizza direttamente la misura invariante $ P $ per studiare il comportamento del sistema all'equilibrio. Per ergodicità, le medie temporali degli osservabili lungo la dinamica coincidono quindi con il valore atteso delle osservabili stesse rispetto alla misura $ P $, cioè coincidono con l'espressione nell'equazione \eqref{eqn:media-osservabile} \\

Un sistema a $ N $ corpi con \emph{spin} è descritto da uno spazio degli stati $ \Sigma = \{-1,1\}^N $ dove $ \sigma_i \in \{-1\, 1\} $ sono le componenti di ciascuno stato $ \sigma $ ($ \sigma_i $ è lo stato di una delle $ N $ particelle del sistema nella configurazione $ \sigma $). Osserviamo preliminarmente che:
\begin{itemize}
    \item se l'hamiltoniana ha la forma $ \ham(\sigma) = \sum_{i=1}^{N} \ham_i(\sigma_i) $, la misura di Boltzmann si fattorizza nel prodotto di $ N $ termini ognuno dei quali dipende da una sola $ \sigma_i $, dunque questa hamiltoniana modellizza un sistema i cui elementi non interagiscono tra loro;
    \item se si aggiungono termini di interazione fra gli elementi (tutti o solo alcuni) si avrà un'hamiltoniana della forma:
    $ \ham(\sigma) = \sum_{i=1}^{N} \ham_i(\sigma_i) + \sum_{i_1, \ldots, i_k} \ham_{i_1, \ldots, i_k} (\sigma_{i_1}, \ldots, \sigma_{i_k}) $.
\end{itemize}
Facciamo ora alcuni esempi di hamiltoniane che modellizzano diversi sistemi fisici.
\begin{description}
    \item[Modello di Ising] Sia $ \Lambda_N = \{-N, \ldots, N\}^d $ un reticolo $ d $-dimensionale e sia lo spazio degli stati $ \Sigma = \{-1,1\}^{\Lambda_N} $. Introduciamo l'hamiltoniana
    \[ \ham(\sigma) = -\frac{1}{2}\sum_{\substack{i,j\in\Lambda_N\\ \abs{i-j}=1}} \sigma_i\sigma_j - \sum_{\substack{i\in\Lambda_N\\ j\in\partial\Lambda_N\\ \abs{i-j}=1}} \sigma_i b_j - \sum_{i\in\Lambda_N} h_i\sigma_i. \]
    Il primo termine descrive l'interazione tra due corpi su lati adiacenti del reticolo, il secondo descrive l'interazione dei corpi con il bordo ad essi adiacente, l'ultimo descrive l'interazione di ciascun corpo con un campo esterno.
    \item[Modelli $ p $-spin] $ \ham(\sigma) = -\sum_{i_1, \ldots, i_p} \sigma_{i_1} \cdots \sigma_{i_p} $.
    \item[Modelli in campo medio] $ \ham_{i_1, \ldots, i_k}(\sigma) = -\frac{1}{2} \sum_{i<j}\sigma_i\sigma_j $.
    \item[Anti-ferromagneti] $ \ham(\sigma) = \frac{1}{2} \sum_{i<j} \sigma_i\sigma_j J $.
    \item[Vetri di spin] $ \ham(\sigma) = - \sum_{i,j} J_{ij} \sigma_i\sigma_j $.
\end{description}

\subsection{Potenziali termodinamici}
Introduciamo l'\emph{energia libera}
\begin{equation}\label{eq:en_libera}
    F_\beta \coloneqq -\frac{1}{\beta} \log{Z_\beta},
\end{equation}
l'\emph{energia interna}
\begin{equation}
    U[P] \coloneqq \langle\ham\rangle = \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma)
\end{equation}
e il funzionale \emph{entropia}
\begin{equation}
    S[P] \coloneqq -\sum_{\sigma\in\Sigma} P(\sigma) \log{P(\sigma)}.
\end{equation}
Calcoliamo ora l'entropia della misura di Boltzmann:
\[ S[P_\beta] = - \sum_{\sigma\in\Sigma} \frac{1}{Z_\beta}e^{-\beta\ham(\sigma)} \left( -\log{Z_\beta}  - \beta\ham(\sigma) \right) = \log{Z_\beta} + \beta U_\beta \]
da cui, usando la \eqref{eq:en_libera}, si ottiene
\begin{equation} \label{eqn:gibbs-boltzmann}
    F_\beta = U_\beta - \frac{1}{\beta} S[P_\beta].
\end{equation}
\begin{equation}
    U_\beta = \pd{\beta F_\beta}{\beta} \qquad S_\beta = \beta^2 \pd{F_\beta}{\beta}
\end{equation}

\subsection{Energia libera di Gibbs e principi variazionali}
Con lo spirito di generalizzare la \eqref{eq:en_libera}, introduciamo il funzionale \emph{energia libera di Gibbs} per una misura di probabilità generica:
\begin{equation}
    G[P] \coloneqq U[P] - \frac{1}{\beta} S[P] = \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma) + \frac{1}{\beta}\sum_{\sigma\in\Sigma} P(\sigma) \log{P(\sigma)}.
\end{equation}
Ovviamente per l'equazione \eqref{eqn:gibbs-boltzmann} si ha $ G[P_\beta] = F_\beta $. Vogliamo ora analizzare se $ G[P_\beta] $ sia un estremo dell'energia libera di Gibbs. A tale scopo introduciamo la \emph{divergenza di Kullback-Leibler}
\begin{equation}
    D(P_1 \parallel P_2) \coloneqq \sum_{\sigma\in\Sigma} P_1(\sigma) \log{\frac{P_1(\sigma)}{P_2(\sigma)}}.
\end{equation}
Si verifica che
\begin{itemize}
    \item $ D(P_1 \parallel P_2) $ è convessa in $ P_1 $;
    \item $ D(P_1 \parallel P_2) \geq 0 \ \forall P_1, P_2 $ ed è nulla se e solo se $ P_1 = P_2 $;
    \item $ D(P_1 \parallel P_2) $ non è simmetrica nei suoi argomenti.
\end{itemize}
Infine si ha che, per una generica $ P $:
\begin{align*}
	G[P] & = \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma) + \frac{1}{\beta}\sum_{\sigma\in\Sigma} P(\sigma) \log{\left(\frac{P(\sigma)}{P_\beta(\sigma)} P_\beta(\sigma)\right)}                \\
	     & = \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma) + \frac{1}{\beta}\sum_{\sigma\in\Sigma} P(\sigma) \log{P_\beta(\sigma)} + D(P \parallel P_\beta)                                      \\
	     & = \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma) + \frac{1}{\beta}\sum_{\sigma\in\Sigma} P(\sigma) \log{\left(\frac{e^{-\beta \ham(\sigma)}}{Z_\beta}\right)} + D(P \parallel P_\beta) \\
	     & = \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma) - \sum_{\sigma\in\Sigma} P(\sigma)\ham(\sigma) - \frac{1}{\beta} Z_\beta \sum_{\sigma\in\Sigma} P(\sigma) + D(P \parallel P_\beta)    \\
	     & = -\frac{1}{\beta} Z_\beta + D(P \parallel P_\beta)
\end{align*}
ovvero
\begin{equation}
     G[P] = G[P_\beta] + \frac{1}{\beta} D(P \parallel P_\beta)
\end{equation}
Pertanto otteniamo che $ G[P] $ ha un minimo (globale) quando $ P = P_\beta $ e, poiché dalla convessità di $ D $ segue che $ G[P] $ è convesso in $ P $, non esistono altri estremi del funzionale. \\

Vogliamo ora riscrivere il principio variazionale così individuato (cioè che la misura di Boltzmann è l'unica che minimizza l'energia libera di Gibbs) in termini di un altro principio variazionale che coinvolga l'entropia. Abbiamo che, detto $ \mathcal{P} $ l'insieme delle possibili misure di probabilità su $ \Sigma $:
\begin{align*}
    P_\beta(\sigma) = \argmin_{P\in\mathcal{P}} G[P] & = \argmin_{P\in\mathcal{P}} \left[ \sum_{\sigma\in\Sigma} P(\sigma) \ham(\sigma) - \frac{1}{\beta} S[P] \right]\\
                                                     & = \argmax_{P\in\mathcal{P}} \left[ S[P] - \beta \sum_{\sigma\in\Sigma}P(\sigma)\ham(\sigma) \right]\\
                                                     & = \argmax_{\substack{P\in\mathcal{P}\\ U=\overline{U}}} S[P].
\end{align*}
Nell'ultimo passaggio si è interpretato $ \beta $ come un moltiplicatore di Lagrange, e dunque il problema di massimo in analisi può essere visto come un problema di massimo della sola entropia, col vincolo di mantenere l'energia interna fissata a una certa costante $ \overline{U} $.

Verifichiamo ora esplicitamente che la misura di Boltzmann è quella che massimizza l'entropia a fissata energia interna. Introduciamo due moltiplicatori di Lagrange per fissare il vincolo che l'energia interna sia pari a $ \overline{U} $ e che la generica misura $ P\in\mathcal{M} $ sia di probabilità, cioè $ \sum_{\sigma\in\Sigma}P(\sigma) = 1 $.
\begin{align*}
    \argmax_{\substack{P\in\mathcal{P}\\ U=\overline{U}}} S[P] & = \argmax_{\substack{P\in\mathcal{P}\\ U=\overline{U}}} \left[ -\sum_{\sigma\in\Sigma}P(\sigma)\log{P(\sigma)} \right]\\
                                                          & = \argmax_{P\in\mathcal{M}} \left[ -\sum_{\sigma\in\Sigma}P(\sigma)\log{P(\sigma)} - \beta \sum_{\sigma\in\Sigma}P(\sigma)\ham(\sigma) + \lambda \sum_{\sigma\in\Sigma}P(\sigma) \right]
\end{align*}
Detto $ p_i \coloneqq P(\sigma^i) $, dove $ \sigma^i \in \Sigma $ è una delle possibili configurazioni, ed essendo $ \Sigma $ finito, possiamo identificare $ \mathcal{M} $ con $ \R^N $, con $ N \coloneqq \card(\Sigma) $. L'argomento dell'$ \argmax $ può quindi essere pensato come una funzione $ f\colon \R^{N+2}\to\R $ così definita
\[ f(p_1, \ldots, p_N, \beta, \lambda) \coloneqq -\sum_{i=1}^N p_i\log{p_i} - \beta \sum_{i=1}^N p_i\ham(\sigma^i) + \lambda \sum_{i=1}^N p_i. \]
Dobbiamo quindi imporre che le derivate parziali $ \pd{f}{p_j} $ si annullino
\[ \lambda -1 - \log{p_j} - \beta \ham(\sigma^j) = 0 \]
da cui
\[ p_j = e^{\lambda - 1} e^{-\beta\ham(\sigma^j)}. \]
Dobbiamo ora imporre i vincoli. Scegliamo $ \lambda $ in modo tale che $ \sum_{i=1}^{N} p_i = 1 $, cioè $ {e^{\lambda - 1} = 1/Z_\beta} $. Imponiamo infine:
\[ \sum_{i=1}^{N} \frac{1}{Z_\beta} e^{-\beta\ham(\sigma^i)} \ham(\sigma^i) = \overline{U} \]
Da cui si ottiene $ \beta $ in funzione di $ \overline{U} $, ma per i nostri scopi conviene lasciare la dipendenza esplicita da $ \beta $. Otteniamo quindi, come atteso,
\[ P(\sigma^j) = \frac{1}{Z_\beta} e^{-\beta\ham{\sigma^j}}. \]

\subsection{Esercizi}

\begin{exercise}
    Sia $ \Sigma $ lo spazio degli stati, $ \ham \colon \Sigma \to \R $ una hamiltoniana, $ P_\beta(\sigma) $ la distribuzione di Boltzmann associata ad $ \ham $ e $ \Sigma_\mathrm{min} \coloneqq \{\argmin_{\sigma \in \Sigma} H(\sigma)\} \subseteq \Sigma $. Mostrare che
    \begin{enumerate}
        \item $ \displaystyle \lim_{\beta \to +\infty} P_\beta(\sigma) = \begin{cases}
        1/\card{(\Sigma_\mathrm{min})} & \text{se $ \sigma \in \Sigma_\mathrm{min} $} \\
        0 & \text{altrimenti}
        \end{cases} $
        \item $ \displaystyle \lim_{\beta \to +\infty} F_\beta = \min_{\sigma \in \Sigma} \ham(\sigma) $
    \end{enumerate}
    dove $ F_\beta $ è l'energia libera.
\end{exercise}
\begin{solution}
    Osserviamo che essendo $ Z_\beta = \sum_{\sigma \in \Sigma} e^{-\beta \ham(\sigma)} $, per $ \beta \to +\infty $ il termine dominante nella somma è quello che massimizza l'esponente cioè $ e^{-\beta \ham(\overline{\sigma})} $ con $ \overline{\sigma} \in \Sigma_\mathrm{min} $. Pertanto
    \[
        Z_\beta \sim \sum_{\overline{\sigma} \in \Sigma_\mathrm{min}} e^{-\beta \ham(\overline{\sigma})} = \card{(\Sigma_\mathrm{min})} \, e^{-\beta \ham(\overline{\sigma})}.
    \]
    Così se $ \sigma \in \Sigma_\mathrm{min} $ allora $ H(\sigma) = H(\overline{\sigma}) $ e quindi
    \[
        P_\beta(\sigma) = \frac{1}{Z_\beta} e^{-\beta \ham(\sigma)} \sim \frac{e^{-\beta \ham(\overline\sigma)}}{\card{(\Sigma_\mathrm{min})} \, e^{-\beta \ham(\overline{\sigma})}} = \frac{1}{\card{(\Sigma_\mathrm{min})}}.
    \]
    Se invece $ \sigma \notin \Sigma_\mathrm{min} $ allora $ H(\sigma) - H(\overline{\sigma}) > 0 $ e quindi
    \[
    P_\beta(\sigma) = \frac{1}{Z_\beta} e^{-\beta \ham(\sigma)} \sim \frac{e^{-\beta \ham(\sigma)}}{\card{(\Sigma_\mathrm{min})} \, e^{-\beta \ham(\overline{\sigma})}} = \frac{1}{\card{(\Sigma_\mathrm{min})}} e^{-\beta[\ham(\sigma) - \ham(\overline \sigma)]} \to 0.
    \]
    Per quanto riguarda l'energia libera abbiamo invece che, sempre per $ \beta \to +\infty $
    \[
        F_\beta = -\frac{1}{\beta} \log Z_\beta \sim -\frac{1}{\beta} \log\left(\card{(\Sigma_\mathrm{min})} \, e^{-\beta \ham(\overline{\sigma})}\right) = -\frac{\log(\card{(\Sigma_\mathrm{min})})}{\beta} + \ham(\overline{\sigma}) \to \ham(\overline{\sigma})
    \]
    che è quanto volevamo essendo per definizione $ \overline{\sigma} \in \Sigma_\mathrm{min} $ e quindi $ H(\overline{\sigma})= \min_{\sigma \in \Sigma} H(\sigma) $.
\end{solution}

\begin{exercise}
    Si consideri un sistema di $ n $ spin $ \pm 1 $ non interagenti con campo esterno uniforme $ h $, i.e. descritto dall'hamiltoniana
    \[ \ham(\sigma) -h \sum_{i=1}^{n} \sigma_i\]
    dove $ \sigma \in \Sigma \coloneqq \{-1, 1\}^n $ e $ \sigma_i $ sono le componenti del vettore $ \sigma $. Calcolare energia libera, energia interna ed entropia del sistema usando le definizioni e poi verificare la relazione $ f_\beta = u_\beta - \beta^{-1} s_\beta $ con $ f_\beta = F_\beta/n $, $ u_\beta = U_\beta/n $ e $ s_\beta = S[P_\beta]/n $ dove $ P_\beta $ è la distribuzione di Boltzmann detti potenziali termodinamici intensivi. Verificare che in termini della magnetizzazione media $ m = \langle 1/n \sum_{i=1}^{n} \sigma_i \rangle $ si ha
    \[ s_\beta = -\frac{1+m}{2} \log{\left(\frac{1+m}{2}\right)} - \frac{1-m}{2} \log{\left(\frac{1-m}{2}\right)}. \]
\end{exercise}
\begin{solution}
    Prima di tutto calcoliamo la funzione di ripartizione
    \[
        Z_\beta = \sum_{\sigma \in \Sigma} \exp\left(\beta h \sum_{i=1}^{n}\sigma_i\right)
    \]
    Fissata la somma $ \sum_{i=1}^{n}\sigma_i $ contiamo il numero di possibili configurazioni che realizzano tale somma. In ogni caso la somma può variare da $ n $ (tutti i $ \sigma_i = 1 $) a $ -n $ (tutti i $ \sigma_i = -1 $). Dividiamo il caso $ n $ pari e $ n $ dispari.
    \begin{itemize}
        \item Se $ n = 2k $ allora possiamo ottenere solo somme pari $ 2l $ con $ -k \leq l \leq k $. Per realizzare tale somma sono necessari $ k + l $ $ \sigma_i = +1 $ e $ k - l $ $ \sigma_i = -1 $. In totale ci sono $ \frac{(2k)!}{(k+l)! \cdot (k-l)!} = \binom{2k}{k+l} $ possibili configurazioni che danno come somma $ 2l $ da cui
        \[
            Z_\beta = \sum_{l = -k}^{k} \binom{2k}{k+l} e^{\beta h \cdot 2l} = \sum_{l = 0}^{2k} \binom{2k}{l} e^{\beta h \cdot (2l-2k)}.
        \]
        \item Similmente se $ n = 2k+1 $ allora possiamo ottenere solo somme pari $ 2l+1 $ con $ -k \leq l \leq k $. Per realizzare tale somma sono necessari $ k+l+1 $ $ \sigma_i = +1 $ e $ k-l $ $ \sigma_i = -1 $. In totale ci sono $ \frac{(2k+1)!}{(k+l+1)! \cdot (k-l)!} = \binom{2k+1}{k+l+1} $ possibili configurazioni che danno come somma $ 2l+1 $ da cui
        \[
            Z_\beta = \sum_{l = -k}^{k} \binom{2k+1}{k+l+1} e^{\beta h \cdot (2l+1)} = \sum_{l = 0}^{2k+1} \binom{2k+1}{l} e^{\beta h \cdot (2l-2k-1)}.
        \]
    \end{itemize}
    Riassumendo per $ n $ generico si ottiene usando il binomio di Newton
    \[
        Z_\beta = e^{-\beta h n} \sum_{l=0}^{n} \binom{n}{l} \left(e^{2 \beta h}\right)^l = e^{-\beta h n} \left[1 + e^{2\beta h}\right]^{n}.
    \]
    L'energia libera è
    \begin{align*}
        F_\beta & = - \frac{1}{\beta} \log{Z_\beta} = - \frac{1}{\beta} \left[-\beta h n + n \log{\left(1+e^{2\beta h}\right)}\right] = n h - \frac{n}{\beta} \log{\left(e^{\beta h}(e^{\beta h}+e^{-\beta h})\right)} \\
        & = -\frac{n}{\beta} \log{\left(e^{\beta h}+e^{-\beta h}\right)} = \frac{n}{\beta} \log(2\cosh{\beta h})
    \end{align*}
    L'energia interna è
    \[
        U_\beta = \pd{}{\beta} \left(\beta F_\beta\right) = -n \pd{}{\beta} \left(\log{\left(e^{\beta h}+e^{-\beta h}\right)}\right) = -n h \frac{e^{\beta h} - e^{-\beta h}}{e^{\beta h}+e^{-\beta h}} = - nh \tanh{(\beta h)}
    \]
    L'entropia della distribuzione di Boltzmann è
    \[
        S[P_\beta] = \beta^2 \pd{F_\beta}{\beta} = -n \left[\log{(2 \cosh{\beta h})} + \beta h \tanh{(\beta h)}\right].
    \]
    Pertanto i potenziali intensivi sono
    \[
        f_\beta = \frac{1}{\beta} \log(2\cosh{\beta h}) \qquad u_\beta =  -h \tanh{(\beta h)} \qquad s_\beta = -\log{(2 \cosh{\beta h})} - \beta h \tanh{(\beta h)}
    \]
    che soddisfano in modo evidente la relazione $ s_\beta = \beta u_\beta - \beta f_\beta $. \\
    Per quanto riguarda la relazione tra $ s_\beta $ e la magnetizzazione osserviamo che
    \[
        m = \left\langle \frac{1}{n} \sum_{i=1}^{n} \sigma_i \right\rangle = -\frac{1}{nh} \langle \ham(\sigma)\rangle = -\frac{1}{nh} U_\beta = - \frac{u_\beta}{h} = \tanh{(\beta h)}.
    \]
    Usando il fatto che
    \[
        \tanh^{-1}(x) = \frac{1}{2} \log(1+x) - \frac{1}{2} \log(1-x) \qquad \cosh(\tanh^{-1}(x)) = \frac{1}{\sqrt{1 - x^2}}
    \]
    otteniamo la tesi
    \begin{align*}
        s_\beta & = -\frac{1+m}{2}\log{\left(\frac{1+m}{2}\right)} -\frac{1-m}{2}\log{\left(\frac{1-m}{2}\right)} \\
        & = -\frac{1+m}{2}\left[\log{(1+m)} - \log{2}\right] -\frac{1-m}{2}\left[\log{(1-m)} - \log{2}\right] \\
        & = \log{2} - \frac{1}{2}\left[\log{(1+m)} + \log{(1-m)}\right] - \frac{m}{2}\left[\log{(1+m)} - \log{(1-m)}\right] \\
        & = \log{\left(2 \frac{1}{\sqrt{1-m^2}}\right)} - \left[\frac{1}{2}\log{(1+m)} - \frac{1}{2}\log{(1-m)}\right] \cdot m \\
        & = \log{\left(2 \cosh{\beta h}\right)} - \beta h \tanh{\beta h}.
    \end{align*}
\end{solution}

\begin{exercise}
    Si consideri un sistema con due livelli di energia: $ \Sigma = \{1, 2\} $, $ H(1) = \epsilon_1 $ e $ H(2) = \epsilon_2 > \epsilon_1 $. Sia $ \Delta = \epsilon_2 - \epsilon_1 $. Calcolare energia libera, energia interna ed entropia del sistema. Controllare che nel limite di alta e bassa temperatura valgono i seguenti sviluppi asintotici:
    \begin{itemize}
        \item $ U_\beta = \langle H\rangle_\Sigma + O(\beta) $ e $ S[P_\beta] = \log{(\card{\Sigma})} + O(\beta) $ per $ \beta \to 0 $
        \item $ U_\beta = H_{\mathrm{min}} + O(e^{-\beta \Delta}) $ e $ S[P_\beta] = \log{(\card{\Sigma_\mathrm{min}})} + O(e^{-\beta \Delta}) $ per $ \beta \to +\infty $
    \end{itemize}
    dove $ \langle H\rangle_\Sigma $ è il valore medio dell'energia rispetto alla distribuzione uniforme su tutto $ \Sigma $ cioè in questo caso $ \langle H\rangle_\Sigma = 1/2 H(1) + 1/2 H(2) = (\epsilon_1 + \epsilon_2)/2  $. Mostrare che gli sviluppi precedenti valgono in generale per sistemi con $ \card{\Sigma} $ finito, definendo opportunamente il gap $ \Delta $.
\end{exercise}
\begin{solution}
    La funzione di ripartizione è $ Z_\beta = e^{-\beta \epsilon_1} + e^{-\beta \epsilon_2} $ da cui la distribuzione di Boltzmann è
    \[
        P_\beta(1) = \frac{e^{-\beta\epsilon_1}}{e^{-\beta \epsilon_1} + e^{-\beta \epsilon_2}}, \qquad P_\beta(2) = \frac{e^{-\beta\epsilon_2}}{e^{-\beta \epsilon_1} + e^{-\beta \epsilon_2}}.
    \]
    L'energia libera è
    \[
        F_\beta = -\frac{1}{\beta} \log{\left[e^{-\beta\epsilon_1}(1+e^{-\beta\Delta})\right]} = \epsilon_1 - \frac{1}{\beta} \log{\left(1+e^{-\beta\Delta}\right)}.
    \]
    L'energia interna è
    \[
        U_\beta = \frac{\epsilon_1 e^{-\beta\epsilon_1} + \epsilon_2 e^{-\beta \epsilon_2}}{e^{-\beta \epsilon_1} + e^{-\beta \epsilon_2}} = \frac{\epsilon_1 + \epsilon_2 e^{-\beta\Delta}}{1+e^{-\beta\Delta}}.
    \]
    L'entropia della distribuzione di Boltzmann è
    \[
        S[P_\beta] = \beta(U_\beta - F_\beta) = \beta \frac{\epsilon_1 + \epsilon_2 e^{-\beta\Delta}}{1+e^{-\beta\Delta}} - \beta\epsilon_1 + \log{\left(1+e^{-\beta\Delta}\right)}.
    \]
    Per $ \beta \to 0 $ si ha
    \begin{align*}
        U_\beta & = \frac{\epsilon_1 + \epsilon_2 - \epsilon_2 \beta\Delta + o(\beta^2)}{2 - \beta\Delta + o(\beta^2)} = \frac{1}{2}(\epsilon_1 + \epsilon_2 - \epsilon_2 \beta\Delta + o(\beta^2))\left(1 - \frac{\beta \Delta}{2} + o(\beta^2)\right) \\
        & = \frac{\epsilon_1 + \epsilon_2}{2} - \frac{\epsilon_1 + 3\epsilon_2}{4} \Delta \beta + o(\beta^2) = \frac{\epsilon_1 + \epsilon_2}{2} + O(\beta) \\
        & \ \\
        F_\beta & = \epsilon_1 - \frac{1}{\beta} \log{\left(2 - \beta\Delta + o(\beta^2)\right)} = \epsilon_1 - \frac{\log{2}}{\beta} - \frac{1}{\beta}\log{\left(1 - \frac{\beta\Delta}{2} + o(\beta^2)\right)} \\
        & = \epsilon_1 - \frac{\log{2}}{\beta} - \frac{\Delta}{2} + o(\beta) \\
        & \ \\
        S[P_\beta] & = \beta(U_\beta - F_\beta) = \beta \frac{\epsilon_1 + \epsilon_2}{2} + \beta \epsilon_2 + \log{2} + \beta\frac{\Delta}{2} + o(\beta^2) = \log{2} + O(\beta)
    \end{align*}
    Invece per $ \beta \to +\infty $ abbiamo che $ U_\beta \to \epsilon_1 = H_\mathrm{min} $ così
    \[
        U_\beta - \epsilon_1 = \frac{\Delta e^{-\beta\Delta}}{1+e^{-\beta\Delta}}
    \]
    cioè $ U_\beta = \epsilon_1 + O(e^{-\beta\Delta}) $. Per quanto riguarda l'energia libera invece $ F_\beta \to \epsilon_1 $ da cui
    \[
        F_\beta - \epsilon_1 = - \frac{1}{\beta} \log{(1+e^{-\beta\Delta})}
    \]
    cioè $ F_\beta = \epsilon + O(e^{-\beta\Delta}/\beta) $ dal momento che $ \frac{\log{(1+e^{-\beta\Delta})}}{e^{-\beta\Delta}} \to 1 $. Pertanto per l'entropia si ha
    \[
        S_\beta = \beta(\epsilon_1 + O(e^{-\beta\Delta}) - \epsilon_1 + O(e^{-\beta\Delta}/\beta)) = O(e^{-\beta\Delta})
    \]
    che è quanto voluto essendo $ \Sigma_\mathrm{min} = \{1\} $ e quindi $ \log{(\card{\Sigma_\mathrm{min}})} = 0 $. \\
    Se ora $ \card{\Sigma} = n \in \N $ l'energia libera è
    \[
        U_\beta = \frac{\sum_{i=1}^{n} \epsilon_i e^{-\beta\epsilon_i}}{\sum_{i=1}^{n}e^{-\beta\epsilon_i}}
    \]
    da cui per $ \beta \to 0 $ essendo $ e^{-\beta\epsilon_i} \sim 1 - \beta\epsilon_i $ si ha
    \[
        U_\beta \sim \frac{\sum_{i=1}^{n} \epsilon_i + \beta \sum_{i=1}^{n} \epsilon_i}{n -\beta \sum_{i=1}^{n} \epsilon_i} \sim \frac{1}{n} \left(\sum_{i=1}^{n}\epsilon_i + \beta \sum_{i=1}^{n} \epsilon_i\right)\left(1 + \frac{\beta}{n}\sum_{i=1}^{n} \epsilon_i\right) \sim \frac{1}{n} \sum_{i=1}^{n}\epsilon_i + O(\beta)
    \]
    mentre per $ \beta \to +\infty $ abbiamo che il termine dominante nelle somme è $ e^{-\beta\epsilon_1} $ essendo $ \epsilon_1 < \epsilon_i $ se $ i > 1 $ che raccogliamo sia a numeratore che a denominatore
    \[
        U_\beta = \frac{\epsilon_1 + \sum_{i=2}^{n}\epsilon_i e^{-\beta\Delta_i}}{1 + \sum_{i=2}^{n}e^{-\beta\Delta_i}}
    \]
    dove abbiamo definito $ \Delta_i = \epsilon_i - \epsilon_1 $. Così $ U_\beta \to \epsilon_1 = H_{\mathrm{min}} $ così
    \[
        U_\beta - \epsilon_1 = \frac{\sum_{i=1}^{n} \Delta_i e^{-\beta\Delta_i}}{1+\sum_{i=2}^{n}e^{-\beta\Delta_i}}.
    \]
    Per $ \beta \to +\infty $, $ e^{-\beta\delta_2} $ è il termine dominante e  quindi concludiamo che $ U_\beta = \epsilon_1 + O(e^{-\beta\Delta_2}) $. Per l'entropia con $ \card{\Sigma} $ finito generico il discorso è analogo.
\end{solution}
