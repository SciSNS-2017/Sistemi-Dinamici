\section{Lezione del 19/12/2018 [Marmi]}

\subsection{Entropia di Shannon}
Claude Shannon: \emph{A Mathematical Theory of Communication}, 1948. \\

Sia $ (X, \mathcal{A}, \mu) $ uno spazio di probabilità e consideriamo una partizione dello spazio \linebreak $ X = A_1 \sqcup \ldots \sqcup A_n \pmod{0} $ cioè tale che gli insiemi $ A_i $ sono a due a due disgiunti a meno di insiemi di misura nulla e la loro unione è $ X $ a meno di un insieme di misura nulla. Sia $ p_i \coloneqq \mu(A_i) $ che definisce un vettore di probabilità $ p \coloneqq (p_1, \ldots, p_n) \in \R^n_+ $ essendo $ \sum_{i = 1}^{n} p_i = 1 $. Indichiamo con $ \Delta^{(n)} \coloneqq \{(p_1, \ldots, p_n) \in \R^n_+ : \sum_{i=0}^{n} p_i\} $ lo spazio dei vettori di probabilità in $ \R^n $.

\begin{thm}[entropia di Shannon]
    Sia $ H^{(n)} \colon \Delta^{(n)} \to [0, +\infty) $ una funzione reale sullo spazio dei vettori di probabilità che chiamiamo entropia. Supponiamo che:    \begin{enumerate}[label=(\roman*)]
        \item $ H^{(n)} $ sia continua e non identicamente costante;
        \item $ H^{(n)} $ sia simmetrica nei suoi argomenti;
        \item $ H^{(n)}(1, 0, \ldots, 0) = 0 $;
        \item le entropie sono "concatenate": $ H^{(n)}(0, p_2, \ldots, p_n) = H^{(n-1)}(p_2, \ldots, p_n) $;
        \item l'entropia massima è quella "più casuale": $ H^{(n)}(p_1, \ldots, p_n) \leq H^{(n)}\left(\dfrac{1}{n}, \ldots, \dfrac{1}{n}\right) $;
        \item l'entropia una partizione in $ n \cdot l $ insiemi è l'entropia degli $ n $ raggruppamenti della partizione in $ l $ insiemi più la media pesata delle "entropie relative" all'interno di ciascun raggruppamento, calcolate con la probabilità condizionata. In formule
        \[
            H^{(nl)}(\pi_{11}, \ldots, \pi_{1l}, \ldots, \pi_{n1}, \ldots, \pi_{nl}) = H^{n}(p_1, \ldots, p_n) + \sum_{i = 0}^{n} p_i \, H^{(l)} \left(\dfrac{\pi_{i1}}{p_i}, \ldots, \dfrac{\pi_{il}}{p_i}\right)
        \]
        dove $ p_i \coloneqq \sum_{j = 1}^{l} \pi_{ij} $.
    \end{enumerate}
    Allora $ \exists C > 0 $ costante positiva tale che
    \[
        H^{(n)}(p_1, \ldots, p_n) = -C \cdot \sum_{i = 1}^{n} p_i \, \log{p_i}
    \]
    e viene detta entropia di Shannon.
\end{thm}
\begin{proof}
    Per prima cosa mostreremo che la tesi vale per $ K(n) \coloneqq H^{(n)}\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) $. Useremo l'ultima proprietà per trovare l'espressione di $ H^{(n)} $ sui razionali e concluderemo così per densità. \textcolor{red}{Mancante}
\end{proof}

Data una partizione $ \mathcal{P} \coloneqq \{A_1, \ldots, A_n\} $ di $ X \pmod{0} $ scriveremo più brevemente $ H(\mathcal{P}) \coloneqq H^{(n)}(\mu(A_1). \ldots, \mu(A_n)) $.

\subsection{Entropia metrica o di Kolmogorov-Sinai}

\begin{definition}[raffinamento di due partizioni]
    Se $ \mathcal{P} \coloneqq \{A_, \ldots, A_n\} $ e $ \mathcal{Q} \coloneqq \{B_1, \ldots, B_m\} $ sono due partizioni di $ X \pmod{0} $ si definisce il raffinamento di $ \mathcal{P} $ e $ \mathcal{Q} $ la partizione
    \[
        \mathcal{P} \vee \mathcal{Q} \coloneqq \{A_i \cap B_j : i = 1, \ldots, n \text{ e } j = 1, \ldots, m\}.
    \]
\end{definition}

Dato un sistema dinamico misurabile $ (X, \mathcal{A}, \mu, f) $ possiamo considerare il raffinamento delle iterazioni di $ f $ su $ \mathcal{P} $. Posto $ f^{-k}\mathcal{P} \coloneqq \{f^{-k}(A_i) : A_i \in \mathcal{P}\} $ sia
\[
    \mathcal{P}^{(n)} \coloneqq \mathcal{P} \vee (f^{-1}\mathcal{P}) \vee \cdots \vee (f^{-(n-1)}\mathcal{P}).
\]

\begin{definition}[entropia di Kolmogorov-Sinai]
    Sia $ (X, \mathcal{A}, \mu, f) $ un sistema dinamico misurabile. L'entropia del sistema è
    \[
        h_\mu(f) \coloneqq \sup_\mathcal{P} {\lim_{n \to +\infty} \frac{1}{n}H(\mathcal{P}^{(n)})}
    \]
    dove l'estremo superiore è fatto al variare di $ \mathcal{P} $ partizione finita (o numerabile) di $ X \pmod{0} $.
\end{definition}

Questa è una buona definizione in virtù del seguente
\begin{lemma}[Fekete] \label{lem:fekete}
    Se $ a \colon \N^{*} \to \R $ è una successione subadditiva allora esiste
    \[
        \lim_{n\to+\infty} \frac{a(n)}{n} = \inf_{n\geq1}a(n).
    \]
\end{lemma}
\begin{proof}
    \textcolor{red}{Mancante}
\end{proof}

\begin{exercise}
    Calcolare l'entropia metrica degli \emph{schemi di Bernoulli}.
\end{exercise}
